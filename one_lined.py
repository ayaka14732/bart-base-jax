# BART multi-token mask filling, in one line
# Usage: python one_lined.py 'Can you see the beautiful flowers <mask> alongside the track?'
(lambda collections, itertools, jax, sys, transformers: (lambda nn, np: transformers.logging.set_verbosity_error() or jax.config.update('jax_platform_name', 'cpu') or jax.config.update('jax_default_matmul_precision', jax.lax.Precision.HIGHEST) or (lambda bart: (lambda convert_qkv: (lambda params: (lambda fwd_layer_norm: (lambda fwd_embedding: (lambda fwd_linear: (lambda fwd_attention: (lambda tokenizer: (lambda batch: (lambda src, mask_enc_1d: print(tokenizer.batch_decode(collections.deque(itertools.accumulate(itertools.repeat(None), func=lambda state, _: (lambda i, dst, a: next(iter(())) if a is not None and np.all(a == 2) else (lambda mask_dec_1d: (lambda a: (i + 1, np.hstack((dst, a[..., None])), a))(np.argmax(nn.softmax((lambda params, src, dst, mask_enc, mask_dec, mask_dec_enc: (lambda src: (lambda src: (lambda dst: collections.deque(itertools.accumulate(params['decoder_layers'], func=lambda dst, decoder_layer: (lambda params, src, dst, mask_dec, mask_dec_enc: (lambda dst: (lambda t: fwd_layer_norm(params['final_layer_norm'], fwd_linear(params['ff1'], nn.gelu(fwd_linear(params['ff0'], t))) + t))(fwd_layer_norm(params['cross_attn_layer_norm'], fwd_attention(params['cross_attn'], src, dst, mask_dec_enc) + dst)))(fwd_layer_norm(params['self_attn_layer_norm'], fwd_attention(params['self_attn'], dst, dst, mask_dec) + dst)))(decoder_layer, src, dst, mask_dec, mask_dec_enc), initial=dst), maxlen=1).pop())(fwd_layer_norm(params['decoder_embed_layer_norm'], fwd_embedding(params['embedding'], dst) + params['decoder_embed_positions'][2:dst.shape[-1]+2])))(collections.deque(itertools.accumulate(params['encoder_layers'], func=lambda src, encoder_layer: (lambda params, src, mask_enc: (lambda t: fwd_layer_norm(params['final_layer_norm'], fwd_linear(params['ff1'], nn.gelu(fwd_linear(params['ff0'], t))) + t))(fwd_layer_norm(params['self_attn_layer_norm'], fwd_attention(params['self_attn'], src, src, mask_enc) + src)))(encoder_layer, src, mask_enc), initial=src), maxlen=1).pop()))(fwd_layer_norm(params['encoder_embed_layer_norm'], fwd_embedding(params['embedding'], src) + params['encoder_embed_positions'][2:src.shape[-1]+2])))(params, src, dst, np.einsum('bi,bj->bij', mask_enc_1d, mask_enc_1d)[:, None], np.tril(np.einsum('bi,bj->bij', mask_dec_1d, mask_dec_1d))[:, None], np.einsum('bi,bj->bij', mask_dec_1d, mask_enc_1d)[:, None]) @ params['embedding']['embedding'].T)[:, -1], axis=-1)))(np.ones((1, i), dtype=np.bool_)))(*state), initial=(1, np.zeros((1, 1), dtype=np.int32), None)), maxlen=1).pop()[1], skip_special_tokens=True)[0]))(batch.input_ids, batch.attention_mask.astype(np.bool_)))(tokenizer([sys.argv[1]], return_tensors='jax')))(transformers.BartTokenizer.from_pretrained('facebook/bart-base')))(lambda params, src, dst, mask: fwd_linear(params['ff'], (lambda a: (lambda d: a.reshape(-1, d[-2] * d[-1]))(a.shape))(np.einsum('bhkv,bvhm->bkhm', np.where(mask, nn.softmax(np.where(mask, np.einsum('bkhm,bvhm->bhkv', fwd_linear(params['q_proj'], dst), fwd_linear(params['k_proj'], src)) / np.sqrt(params['q_proj']['kernel'].shape[-1]), np.NINF)), 0), fwd_linear(params['v_proj'], src))))))(lambda params, x: np.dot(x, params['kernel']) + params['bias']))(lambda params, x: params['embedding'][x]))(lambda params, x: ((x - x.mean(-1, keepdims=True)) / np.sqrt(x.var(-1, keepdims=True) + 1e-5)) * params['scale'] + params['bias']))({'embedding': {'embedding': bart['shared']['embedding']}, 'encoder_embed_positions': bart['encoder']['embed_positions']['embedding'], 'decoder_embed_positions': bart['decoder']['embed_positions']['embedding'], 'encoder_embed_layer_norm': bart['encoder']['layernorm_embedding'], 'decoder_embed_layer_norm': bart['decoder']['layernorm_embedding'], 'encoder_layers': [(lambda params: {'self_attn': {'q_proj': convert_qkv(params['self_attn']['q_proj']), 'k_proj': convert_qkv(params['self_attn']['k_proj']), 'v_proj': convert_qkv(params['self_attn']['v_proj']), 'ff': params['self_attn']['out_proj']}, 'self_attn_layer_norm': params['self_attn_layer_norm'], 'ff0': params['fc1'], 'ff1': params['fc2'], 'final_layer_norm': params['final_layer_norm']})(bart['encoder']['layers'][str(i)]) for i in range(6)], 'decoder_layers': [(lambda params: {'self_attn': {'q_proj': convert_qkv(params['self_attn']['q_proj']), 'k_proj': convert_qkv(params['self_attn']['k_proj']), 'v_proj': convert_qkv(params['self_attn']['v_proj']), 'ff': params['self_attn']['out_proj']}, 'self_attn_layer_norm': params['self_attn_layer_norm'], 'cross_attn': {'q_proj': convert_qkv(params['encoder_attn']['q_proj']), 'k_proj': convert_qkv(params['encoder_attn']['k_proj']), 'v_proj': convert_qkv(params['encoder_attn']['v_proj']), 'ff': params['encoder_attn']['out_proj']}, 'cross_attn_layer_norm': params['encoder_attn_layer_norm'], 'ff0': params['fc1'], 'ff1': params['fc2'], 'final_layer_norm': params['final_layer_norm']})(bart['decoder']['layers'][str(i)]) for i in range(6)]}))(lambda params: {'kernel': params['kernel'].reshape(768, 12, 64).transpose(1, 0, 2), 'bias': params['bias'].reshape(12, 64)}))(transformers.FlaxBartForSequenceClassification.from_pretrained('facebook/bart-base').params['model']))(jax.nn, jax.numpy))(*map(__import__, ('collections', 'itertools', 'jax', 'sys', 'transformers')))
